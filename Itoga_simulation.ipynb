{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from src.IterativeFitting import IterativeFitting as IF\n",
    "from src.CorrFuncs import covariance_matrix, trend_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "study_df = pd.read_excel(\"SBPvPAD_data.xlsx\")\n",
    "\n",
    "# Taking care of non-log values in dataframe\n",
    "study_df.iloc[0,5:8] = np.log(study_df.iloc[0,5:8].to_numpy().astype(np.float64))\n",
    "study_df.iloc[5,5:8] = np.log(study_df.iloc[5,5:8].to_numpy().astype(np.float64))\n",
    "\n",
    "# Creating Itoga-specific dataframe\n",
    "study_df_i = study_df.loc[study_df[\"Author\"] == \"Itoga\"]\n",
    "study_df_i = study_df_i.iloc[1:,:]\n",
    "\n",
    "# Create exposure levels relative to reference exposure\n",
    "x_i = study_df_i[\"dose\"].to_numpy()[1:] - study_df_i[\"dose\"].to_numpy()[0]\n",
    "\n",
    "# Get log-odds and corresponding variance estimates\n",
    "L_i = study_df_i[\"logOR\"].to_numpy()[1:]\n",
    "v_i = study_df_i[\"std_error\"].to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing simulation slope and intercept parameters\n",
    "beta0 = -3.6289\n",
    "beta1 = 0.0246\n",
    "\n",
    "# Generating plausible x values to throw into probability generator\n",
    "xs = np.random.uniform(low=0,high=20,size=40000)\n",
    "\n",
    "# Function to generate probabilities of being a case v non-case\n",
    "p = lambda x: (np.exp(beta0 + beta1*x))/(1 + np.exp(beta0 + beta1*x))\n",
    "\n",
    "# Actually calculating probabilities on exposures as defined\n",
    "px = p(xs)\n",
    "\n",
    "# Actually assigning to case or not\n",
    "outcomes = np.array([np.random.binomial(n=1,p=p,size=1)[0] for p in px])\n",
    "\n",
    "# Constructing and sorting dataframe of outcomes and exposure\n",
    "df = np.stack([outcomes,xs],axis=1)\n",
    "df = df[np.argsort(df[:, 1])]\n",
    "\n",
    "# Observations at each category level\n",
    "C1 = df[df[:,1] < x_i[0]]\n",
    "C2 = df[np.logical_and(df[:,1] >= x_i[0], df[:,1] < x_i[1])]\n",
    "C3 = df[np.logical_and(df[:,1] >= x_i[1], df[:,1] < x_i[2])]\n",
    "C4 = df[np.logical_and(df[:,1] >= x_i[2], df[:,1] < x_i[3])]\n",
    "C5 = df[df[:,1] >= x_i[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting numbers of cases vs non-cases in each category\n",
    "cases1 = np.sum(C1[:,0])\n",
    "noncases1 = C1[:,0].shape[0] - cases1\n",
    "\n",
    "cases2 = np.sum(C2[:,0])\n",
    "noncases2 = C2[:,0].shape[0] - cases2\n",
    "\n",
    "cases3 = np.sum(C3[:,0])\n",
    "noncases3 = C3[:,0].shape[0] - cases3\n",
    "\n",
    "cases4 = np.sum(C4[:,0])\n",
    "noncases4 = C4[:,0].shape[0] - cases4\n",
    "\n",
    "cases5 = np.sum(C5[:,0])\n",
    "noncases5 = C5[:,0].shape[0] - cases5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to get crude OR estimates\n",
    "crude = lambda x,y: (x/y) / (cases1/noncases1)\n",
    "\n",
    "# Actually calculating the crude OR estimates\n",
    "crudeor1 = crude(cases1,noncases1)\n",
    "crudeor2 = crude(cases2,noncases2)\n",
    "crudeor3 = crude(cases3,noncases3)\n",
    "crudeor4 = crude(cases4,noncases4)\n",
    "crudeor5 = crude(cases5,noncases5)\n",
    "\n",
    "# Getting categories defined\n",
    "in_cat1 = np.zeros(C1.shape[0]) + 2\n",
    "in_cat2 = np.zeros(C2.shape[0]) + 3\n",
    "in_cat3 = np.zeros(C3.shape[0]) + 4\n",
    "in_cat4 = np.zeros(C4.shape[0]) + 5\n",
    "in_cat5 = np.zeros(C5.shape[0]) + 6\n",
    "\n",
    "cats = np.append(in_cat1,in_cat2)\n",
    "cats = np.append(cats,in_cat3)\n",
    "cats = np.append(cats,in_cat4)\n",
    "cats = np.append(cats,in_cat5)\n",
    "\n",
    "cats_out_df = np.stack([df[:,0],cats], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting subjects and total number of cases\n",
    "N = np.array([cases1+noncases1,cases2+noncases2,cases3+noncases3,cases4+noncases4,cases5+noncases5])\n",
    "M1 = cases1 + cases2 + cases3 + cases4 + cases5\n",
    "\n",
    "# Initialization\n",
    "A0 = M1*N[1:]/(N.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the log-odds and variance estimates from Itoga directly to construct the covariance matrix for the adjusted method, and we will use them again to estimate the slope coefficient on the standard, non-correlation corrected method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/correlationCorrection/lib/python3.12/site-packages/cvxpy/reductions/solvers/solving_chain.py:354: FutureWarning: \n",
      "    You specified your problem should be solved by ECOS. Starting in\n",
      "    CXVPY 1.6.0, ECOS will no longer be installed by default with CVXPY.\n",
      "    Please either add an explicit dependency on ECOS or switch to our new\n",
      "    default solver, Clarabel, by either not specifying a solver argument\n",
      "    or specifying ``solver=cp.CLARABEL``.\n",
      "    \n",
      "  warnings.warn(ECOS_DEP_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "it_fit_ex = IF(L_i,A0,N,M1)\n",
    "A, B, a0, b0 = it_fit_ex.convexProgram()\n",
    "\n",
    "C = covariance_matrix(A,B,a0,b0,v_i**2)\n",
    "inv_C = np.linalg.inv(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering slope estimate for corrected correlation\n",
    "vb_star = 1/(np.dot(x_i,np.dot(inv_C,x_i)))\n",
    "b_star = vb_star*(np.dot(x_i,np.dot(inv_C,L_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014094088127677437"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovering standard slope estimate\n",
    "vb = 1/(np.dot(x_i,np.dot(np.linalg.inv(np.diag(v_i**2)),x_i)))\n",
    "b = vb*(np.dot(x_i,np.dot(np.linalg.inv(np.diag(v_i**2)),L_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0077151064467757356"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "correlationCorrection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
